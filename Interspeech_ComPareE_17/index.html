<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Huiting Hong</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Homepage and blog">
    <meta name="author" content="Huiting Hong">
    
    <link rel="canonical" href="https://w102060018w.github.io/blog/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Avi Singh" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201602132020" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Verifications -->
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Blog">
    <meta property="og:description" content="Homepage and blog">
    <meta property="og:url" content="https://w102060018w.github.io/blog/">
    <meta property="og:site_name" content="Huiting Hong">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
        <meta name="twitter:site" content="@w102060018w" />
    
    <meta name="twitter:title" content="Blog" />
    <meta name="twitter:description" content="Homepage and blog" />
    <meta name="twitter:url" content="https://w102060018w.github.io/blog/" />

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-63278661-1']);
      _gaq.push(['_trackPageview']);
      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    
</head>

<body class="site">
  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="https://w102060018w.github.io" class="site-title"></a>
      <nav class="site-nav">
        <a href="/">Home</a>
<a href="/research/">Research</a>
<a href="/blog/">Blog</a>
      </nav>
      <div class="clearfix" ></div>
      
        <div class="social-icons">
  <div class="left">
    
      <a class="fa fa-github" href="https://github.com/w102060018w"></a>
    
    
      <a class="fa fa-twitter" href="https://twitter.com/w102060018w"></a>
    
    
      <a class="fa fa-linkedin" href="https://www.linkedin.com/in/huiting-hong-a591a9129/"></a>
    
    
    
  </div>
  <div class="right">

    
    
    
  </div>
</div>

<div class="clearfix">
    
      <center> <a class="fa fa-envelope" href="mailto:w102060018w@gmail.com"></a> w102060018w@gmail.com <center>
    
</div>

    

      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>Interspeech 2017 ComPareE<Down></h1>
  <span class="post-meta">Mar 30, 2017</span><br>
<!--   <span class="post-meta small">
    Basic concept on 
  </span> -->
</div>

<article class="post-content">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<p>I start working on this challenge around Feb. 2017 with other members in our <a href="http://biic.ee.nthu.edu.tw/">lab</a>. We plan to apply several traditional procedures on detecting audio signal with specific characteristic and try other distinctive approaches to see if they will get better results. In this article, I will give a brief concept of different encoding approach, including BOW, GMM along with Fisher Vector. Also I will show some detail on strength model. Finally show the results we get in this challenge. </p>

<h2>Commonly used encoding approaches</h2>
<h4>BOW(Bag of Words)</h4>
<p>Bag of words is an approach which can reduce the dimension of features processed from raw data. The brief concept can be devided into two parts, one is to decide the criteria of categorization and the other is to generate histogram base on the criteria:</br>
<i>Suppose we have already processed our raw data (audio signal) and get the N<sub>x</sub>D dimension features.</i>  
<h5>Decide the criteria of cluster-categorization</h5>
Sometimes we also say it's just like making a vocabulary codebook, which can later be used at counting the amount of each word.<br/> 
The idea started at using <a href="https://www.wikiwand.com/en/K-means_clustering">kmeans</a> method to generate numbers of k centrol points of k-clusters respectively. If nowadays we have mutiple features, F<sub>i</sub>, whose dimension are all N<sub>x</sub>D, we can randomly sample equal amount of rows from F<sub>i</sub> and generate the center point of each cluster base on the information in different features. In this way, the center point we create of each cluster can be more valid on different feature, and let the later categorized-result(histogram) to be more representative and convincing.
<h5>Generate histogram</h5>
After we got the center point of each cluster, we can categorize our features into different clusters. Each features, F<sub>i</sub>, will have a categorization result, showing the amount of vectors belongs to each cluster.(Due to the dimension of F<sub>i</sub> is N<sub>x</sub>D, we can know that there are totally N vectors with length D in the feature F<sub>i</sub>). Finally, the 'voting' result can be seen as a histogram, which x-axis represents k-cluster and y-axis represents the number of vectors belongs to each cluster.</br>
The generated histogram is then used to describe the origianl features, which reduce the dimension from N<sub>x</sub>D to 1<sub>x</sub>D. </br>
<p><b>In short, BOW approach can help us to encode the feature into a more representative while low-dimension result.</b></p>
</br>
Here's the flow:</br>
<img src="/images/Interspeech_2017/BOW.png" alt="model-nlor" /></br>


<h4>GMM and Fisher Vector</h4>
<p>Another much more popularly used encoding aproach is Gaussian Mixture Model(GMM) followed by Fisher Vector. The <a href="https://www.wikiwand.com/en/Mixture_model">GMM</a> is like a more extended version of kmeans-clustering, it represents a cluster not using a center point but a gaussian distribution with correspnding weight. Therefore, with our features, F<sub>i</sub>, in dimension N<sub>x</sub>D, we can expect that we will get the D amount of set of Gaissian Distribution. In each set, there will be the number of K gaussian distribution as we set the number of clusters to be K. We all know that in order to describe a gaussian distribution, we have to specify its mean and variance, so after generating the Gaussian Mixture Model of our k-cluster, we will have a matrix of Mean<sub>DxK</sub> and a matrix of Covarience<sub>DxK</sub>. Additionally, to describe the relationship between k gaussian distribution, we will have the posterior value and a matrix of Prior<sub>Kx1</sub>(which could also be considered as the weight between k gaussian distribution).</br></p>
<p>After we build our GMM, we can now encode our features, F<sub>i</sub>, base on the Mean<sub>DxK</sub>,Covarience<sub>DxK</sub>, posterier, and Prior<sub>Kx1</sub>. The encoded result is what we called Fisher Vector.(a special case of general <a href = "https://www.wikiwand.com/en/Fisher_kernel">Fisher Kernel</a>)</br>
When starting encoding our features, F<sub>i</sub>(<sub>NxD</sub>), we will pick out one feature(N<sub>x</sub>1) at each time, and see which set of gaussian distribution is more similar to itself. After finding out that set, it will modify the prior between K gaussian distribution in the set, in order to fit the feature better. Therefore, the encoded result of one feature is the matrix of prior value along with posterior, which dimension should be K<sub>x</sub>2. After finishing each feature in the F<sub>i</sub>, we will get the Fisher Vector with dimension of D<sub>x</sub>K<sub>x</sub>2.</br>

<p>The Fisher Vector is then used to describe the origianl features. We can see that the original dimension is N<sub>x</sub>D while the encoded result is D<sub>x</sub>K<sub>x</sub>2, usually K will smaller than N, so the encoded reesult will decrease the dimension and since the representation of gaussian distrution is more complicate, use it as the description of a cluster will be more convincing. That is why Fisher Vector is used more popularly when encoding the features.</br></p>
Here's the flow:</br>
<img src="/images/Interspeech_2017/FV.png" alt="model-nlor" />

<h2>Strength Modeling</h2>
<p> Strength model is a kind of fusion method propsed by J. Han, Z. Zhang, N. Cummins, F. Ringeval, and
B. Schuller, which not only consider the fusion of feature level but also decision level, and shows great improvement in fusion perspective.</br></p>
<p><i>Feature Level</i></br>
Before fusion, we use sub-dictionary concept and generate 3 different kind of encoded features, which are label-based approach, unsupervise approach and supervise approach.</br> 
- In label-based approach, we know that we have people with cold and no-cold these two kind of label. We first generate the cold-specifc GMM and no-cold-specific GMM and later use the Fisher Vector mentioned above to generate new encoded features.</br>
<p>Here's the brief flow:</p></br>
<img src="/images/Interspeech_2017/StrengthModel_label.png" alt="model-nlor" /></br>
- In unsupervise approach, we apply kmeans method and categorize the data into two categories which give the data another meaning, like men or women for example. After splitting out the two groups(clusters), we apply the same procedure: first generate GMM for each cluster data and then use Fisher Vectore to encode features.</br>
<p>Here's the brief flow:</p></br>
<img src="/images/Interspeech_2017/StrengthModel_unspv.png" alt="model-nlor" /></br>
- In supervise approach, it will be a little bit more complicated. The main idea is to find out those data whose connection between its label and characteristic is super low and hard to classify. The way to find out is first put train data into classifier, like SVM, and then put train data as test data to see its prediction result. We could imagine the UAR must be super high due to the testing part is using the train data which the machine have already seen in training process. However, there must exist some data still get a wrong prediction result, and that is what we are looking for! After finding out these data, we base on their prediction result and generate GMM respectively. Finally apply Fisher Vector to get another encoded features.</br>
<p>Here's the brief flow:</p></br>
<img src="/images/Interspeech_2017/StrengthModel_spv.png" alt="model-nlor" /></br></p>

<p><i>Decision Level</i></br>
In addition to feature level encoded result mentioned above, we still need to obtain decision level outcomes</br>
Due to different classifiers will provide different perspective of the problem, we choose 4 well-known classifiers, including SVM, adaboost, random forest, and naive bayes. Then take these classifiers' prediction result as the decison level outcome which will be fusion later.</p>

<p>
In fusion process, we then combine the feature level and decision level as the traning input of our classifier, and hope it will learn differnt perspective and characteristic of data, them predict better result.</p>

<h2>Our Approach and Result</h2>
<p>Combine sub-dictionary features, decision level features and eGeMAPS functional features, and optimize each feature with different classifiers. The result is as follows:</br>
<img src="/images/Interspeech_2017/Final_result.png" alt="model-nlor" /></br></p>
We can see that with strength model, the performance gets better. Also, combine the sub-dictionary approach and eGeMAPS functional features, it gets quite a convincing result.


<!-- 
Here's the illustration: </p>
<p><img src="/images/retrieval/illustration.png" alt="illustration" /></p>
<p>The blue box represents the ground truth, while the yellow stands for positive recall, red for negative recall.</p>

<h2>Overview:</h2>
<ol>
  <li>Natural Language Object Retrieval </li>
  <li>Grounding with supervised training + multi-task loss</li>
  <li>Grounding + region proposal network</li>
</ol>

<h4>1. Natural Language Object Retrieval, inspired by <a href="http://arxiv.org/abs/1511.04164">Ronghang Hu's work</a></h4>
<p>This work was appeared in cvpr 2016. It extend the previous work, <a href="https://arxiv.org/abs/1411.4389">LRCN</a>, use 
the same way to generate caption. Compare the predicted caption with the query, and find the most related one. I call the whole 
process "reconstruction", which means that the model try to reconstruct back the query based on the bbox feature.</br>
Here's the model:</br>
<img src="/images/retrieval/natural-language-object-retrieval-model.png" alt="model-nlor" />
Noted that the model generate the caption not onlt based on the bbox feature, but also on the context feature, though it's a relatively
 naive way to use the context information. The paper also mentions the w/o context feature perormance:</br>
<img src="/images/retrieval/exp.png" alt="exp-nlor" /></p>

<h4>2. Grounding with supervised training + multi-task loss, inspired by <a href="https://arxiv.org/abs/1511.03745">
    Anna Rohrbach's work</a></h4>
<p>This work is from the same team of UC Berkeley. It extends the work to three different models, unsupervised, semi-supervised, and 
fully supervised model. Basically, it also uses the concept of reconstruction and combine it with a more direct way: directly choose
 the most related region. Furthermore, in the unsupervised model, they also use the attention mechanism, which is used heavily in 
 image caption task.<\br>
Here, I modified the fully supervised method with an extra reconstruction loss. This make the whole model become like a multi-task 
model, which increase the performance about 2%! The following is what the model look like:<\br>
<img src="/images/retrieval/model2.png" alt="model2" />
</p>

<h2>Acknowledgement:</h2>
<p>This work is done during my Umbo CV internship. Thanks to all the umbots that fulfills my summer internship. Umbo CV really 
provides a comfortable environment for coders. I truely enjoy the life there. :)
</p>
-->
<h2>Reference:</h2>
H. Kaya and A. A. Karpov, <a href = "https://www.researchgate.net/publication/307889338_Fusing_Acoustic_Feature_Representations_for_Computational_Paralinguistics_Tasks" >Fusing acoustic feature representations for computational paralinguistics tasks</a> Interspeech 2016, pp. 2046–2050, 2016.<br>
J. Han, Z. Zhang, N. Cummins, F. Ringeval, and B. Schuller, <a href = "http://www.sciencedirect.com/science/article/pii/S0262885616302177"> Strength modelling for real-worldautomatic continuous affect recognition from audiovisual signals</a>, Image and Vision Computing, pp. –, 2016.</br>
F.A.Laleye, E.C.Ezin, and C.Motamed, <a href = "https://link.springer.com/chapter/10.1007/978-3-319-31898-1_4">Speechphonemeclas- sification by intelligent decision-level fusion</a> in Informatics in Control, Automation and Robotics 12th International Conference, ICINCO 2015 Colmar, France, July 21-23, 2015 Revised Selected Papers. Springer, 2016, pp. 63–78.</a></br>
N. Zhou and J. Fan, <a href = "http://ieeexplore.ieee.org/document/6616553/"> Jointly learning visually correlated dictionaries for large-scale visual recognition applications</a>, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 4, pp. 715–730, April 2014.</br>
F. Scalzo, G. Bebis, M. Nicolescu, L. Loss, and A. Tavakkoli, <a href = "https://www.cse.unr.edu/~bebis/FabienICPR08.pdf"> Feature fusion hierarchies for gender classification</a>, in Pattern Recognition, 2008. ICPR 2008. 19th International Conference on IEEE, 2008, pp. 1–4.</br>
M. Liu, D. Zhang, and D. Shen, <a href = "https://www.ncbi.nlm.nih.gov/pubmed/23417832"> Hierarchical fusion of features and classifier decisions for alzheimer’s disease diagnosis</a>, Human brain mapping, vol. 35, no. 4, pp. 1305–1319, 2014.</br>
F. Eyben, F. Weninger, F. Gross, and B. Schuller,<a href = "http://doi.acm.org/10.1145/2502081.2502224"> Recent developments in opensmile, the munich open-source multimedia feature extractor</a>, in Proceedings of the 21st ACM International Conference on Multimedia, ser. MM ’13. New York, NY, USA: ACM, 2013, pp. 835–838.</br>
Chia-Lung Wu1, Hsiang-Ping Hsu1, Xian-Feng Liao2, Yu Tsao2, Hao-Chun Yang3, Jen-Lin Li3, Chi-Chun Lee3, Hung-Shin Lee4 and Hsin-Min Wang4, "Automatic Detection of Speech under Cold using Discriminative Autoencoders and Strength Modeling with Multiple Sub-Dictionary Generation"</br>


</article>

  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'http-w102060018w-github-io';
    var disqus_identifier = '/Interspeech_ComPareE_17';
    var disqus_title      = '';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Powered by <a href="https://jekyllrb.com/">Jekyll</a> using <a href="https://pixyll.com/">Pixyll</a> theme. Hosted on <a href="https://pages.github.com/">Github Pages</a>. <br>
      © Yuan-Hong Liao
    </small>
  </div>
</footer>

</body>
</html>
